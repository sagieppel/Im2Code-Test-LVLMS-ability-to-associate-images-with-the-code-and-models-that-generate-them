import os
import numpy as np
import random
import cv2
from tools import clean_code_comments
import tools.VisualQuestion as VQ
import json_pkl
#Im2Code: Test LVLMS abitility to associate images to the code and models that generate them
# This is done using  a multi choice question where the LVLM receive code or model verbal description
# and several images generated by the different models and is asked to find which images where generated by the described model
# This script run on the SciTextures dataset.
# Run Im2Code and Im2Desc tests:
################################Run  image to image matching test ###########################################################################################

def run_test_text2im(code_main_dir,
                     images_main_dir, # images main dir (Scitexture/images/
                     relative_image_path=False, # True if the image dir is a subdir of the code dir
                     num_choices=10, # number of choices in the multichoice tes
                     max_question=100, # Max questions per text
                     num_samples=1, # Number of referance image per model
                     mode="code", # "description" # match images to code or to model description
                     display=False, # for debug
                     error_dir="",# error dir where mistakes will be saved
                     remove_code_comments=False, # Remove comments from code
                     outdir="", # output dir
                     single_image=True, # Merge all images into single image (recomanded) not all LVLMs can deal with many images)
                     model=""):
    results={"correct":0,"wrong":0,"fail":0}
#-----------------Create file structure (dictionary containing images and associated code)------------------------------------------
    im_data={}
    txt_data={}
    for dr in os.listdir(code_main_dir):
        if relative_image_path: # if image dir is subdir of code dir
                  img_dir= code_main_dir + "//" + dr + "//" + images_main_dir
        else:
            img_dir= images_main_dir + "//" + dr + "//"
        if mode=="code": # match image to code
            file_path = code_main_dir + "//" + dr + "//generate.py"
        else: # match image to description
            file_path = code_main_dir + "//" + dr + ("//Description.txt")
        if not os.path.exists(file_path):
              continue

        if not os.path.isdir(img_dir)  or len(os.listdir(img_dir))<num_samples: continue

        with open(file_path, "r") as fl:
            txt_data[dr] = fl.read()
            if mode == "code" and remove_code_comments:
                txt_data[dr] = clean_code_comments.remove_non_executable_code(txt_data[dr])
        im_data[dr]=[]

        for fl in os.listdir(img_dir):
            if ".jpg" in fl or ".png" in fl:
                 im_data[dr].append(img_dir + fl)


#-------------------run test-----------------------------------------------------------------------
    all_grps = list(im_data.keys())

    for  n_qst,pos_grp in enumerate(all_grps):
        if n_qst>max_question:break
        used_groups = [pos_grp]
        test_images = {}
    #---------------------------------------------------------------------------------------
        correct_indx=np.random.randint(num_choices)+1 # index of postive choice
        correct_choice = "group_" + str(correct_indx)
        options=[]

        for ii in range(1,num_choices+1):
            options.append("group_"+str(ii))
            if ii==correct_indx:
                grp = pos_grp
            else:
                while (True):
                    grp = random.sample(all_grps, 1)[0]
                    if grp not in used_groups:
                        used_groups.append(grp)
                        break

    #----------------------sample  referance------------------------------------------------------------

            samples = random.sample(im_data[grp],num_samples)
            for kk,sm in enumerate(samples):
                test_images["group_"+str(ii)+"_sample_"+str(kk+1)] = sm #im_data[grp][sm]


    #-------------------------Create prompt for LVLM---------------------------------------------------------------------------

        txt=("\n I want you to identify which of the following textures and visual patterns was generated by the code."
             "\nYou are given set of images containing several group of patterns "+str(options)+
             "\nYou are also given the following "+mode+": \n***\n" + txt_data[pos_grp]+"\n***"+
            "\n ")

        if mode == "code":
            txt += ("Code that generate specific type of visual patterns. "
                    "\n One and only one of the group of patterns in the images were generated by this code")
        else:
            txt += ("description of specific type of visual patterns"
                    "\n One and only one of the group of patterns in the images were generated by the process in the text description")

        txt +=  ("\n look carefully at the test images and which one is it."
            "\n Your answer should come as json dictionary of the following structure:\n"
                   " {'answer':the exact answer which have to be one of the following: "+str(options)+
                   "\n, 'Explanation': Free text Explanation of your choice}")
#--------------------------------------------------------------------------------------------------------------------------------
        # --------------Unify images (optional)----------------------------------------------
        if single_image:
            im_path = ""
            if len(outdir) > 0:
                if not os.path.exists(outdir): os.mkdir(outdir)
                im_path = outdir + "//" + str(n_qst) + ".jpg"
                json_pkl.save_json(test_images, outdir + "//" + str(n_qst) + ".json")
            im, test_images = VQ.unify_image(data=test_images, num_columns=6, labels=["group"],#options,#["reference", "test"],
                                             out_file=im_path, disp=False)

        #----------------Submit query to LVLM----------------------------------------------------------------------------------------------------------------------
        outcome= "fail"
        print(txt)
        for i in range(4):
         #  ans=F3.get_response_image_json(text=txt,img_path=test_images, model="gpt-5")
           ans = VQ.get_response_image_txt_json(text=txt, img_path=test_images, model=model)
           print(ans)
           if 'answer' in ans and ans['answer'] in options:
               if ans['answer']==correct_choice:
                   outcome= "correct"
               else:
                   outcome= "wrong"
               break
        print(outcome)

        results[outcome] += 1

        print(results)
        # ***************************Display and save **********************************************************
        # ================== save============
        if len(error_dir) > 0:# and (outcome == "wrong" or outcome == "fail"):
            if not os.path.exists(error_dir): os.mkdir(error_dir)
            spec_er_dir = os.path.join(error_dir, pos_grp)
            if not os.path.exists(spec_er_dir): os.mkdir(spec_er_dir)
            for ky in test_images:
                if ky == correct_choice:
                    cv2.imwrite(spec_er_dir + "//" + ky + "correct.jpg", cv2.imread(test_images[ky]))
                elif ("answer" in ans) and ky == ans["answer"]:
                    cv2.imwrite(spec_er_dir + "//" + ky + "choice.jpg", cv2.imread(test_images[ky]))
                else:
                    cv2.imwrite(spec_er_dir + "//" + ky + ".jpg", cv2.imread(test_images[ky]))
            with open(spec_er_dir + "//data.txt", "w") as fl:
                fl.write(pos_grp + "\n" + str(ans) + "\ncorrect" + correct_choice)
        #-----------------------------------------------------------------------------------
        # display
        if display:
            for ky in test_images:
                if ky == correct_choice:
                    cv2.imshow(ky + "correct", cv2.imread(test_images[ky]))
                else:
                    cv2.imshow(ky + "  " + grp, cv2.imread(test_images[ky]))
                    print(grp)
            cv2.waitKey()
        # =*******************************************************
        #--------------------Calculate accuracy-----------------------------------------------------------------

    results["accuracy"] = results["correct"]/(results["wrong"]+results["fail"]+results["correct"])
    return results

##############################################################################################################

#                 Run test with multiple models

##############################################################################################################################
def run_test_multi_model(
        code_main_dir,images_main_dir,
        main_outdir,
        max_questions=100,
        num_reference=3,
        num_choices=10,
        single_img=True,
        mode="description",#"code",#"code",  # description
        remove_code_comments=False,
        skip_exist=True
):
    if not os.path.exists(main_outdir):
        os.mkdir(main_outdir)
    openai_models = ["gpt-5-mini", "gpt-5"]  # , "gpt-oss-120b", "gpt-oss-20b"]
    together_models = [ "Qwen/Qwen2.5-VL-72B-Instruct","google/gemma-3n-E4B-it", "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8","meta-llama/Llama-4-Scout-17B-16E-Instruct"]
    gemini_models = ["gemini-2.5-pro", "gemini-2.5-flash"]
    grok_models = ["grok-4-fast-reasoning", "grok-4-fast-non-reasoning", "grok-4"]
    claude_models = ["claude-sonnet-4-5-20250929"]
    combine_list = openai_models+gemini_models+together_models+gemini_models+grok_models#+claude_models

    for model in combine_list:
        model_simple_name = model.replace(".", "").replace(" ", "").replace("-", "_").replace("/","_").replace(r"\\",r"_")
        if os.path.exists(main_outdir + "//" + model_simple_name + ".pkl") and skip_exist: continue
        stats = run_test_text2im(
                         code_main_dir,images_main_dir,
                         max_question=max_questions,
                         num_choices=num_choices,
                         num_samples=num_reference,
                         mode=mode,  # description
                         remove_code_comments=remove_code_comments,
                         display=False,
                         model=model,
                         error_dir=main_outdir + "//" + model_simple_name + "Fail//",
                         single_image=single_img,
                         outdir=main_outdir + "//" + model_simple_name)

        json_pkl.save_json(stats,main_outdir+"//"+model_simple_name+".json")
        json_pkl.save_pkl(stats, main_outdir + "//" + model_simple_name + ".pkl")
        print(main_outdir+"//"+model_simple_name)



############################################main ###########################################################################################
# ----------------------------- __main__ -----------------------------

if __name__ == "__main__":
    # >>>>> EDIT PATH BELOW TO YOUR DATA ROOT <<<<<
    code_main_dir = r"Scitexture/code_and_data/" # Code main dir from the SciTextures dataset
    images_main_dir = r"Scitexture/images/" # Image main dir from the SciTextures dataset
    out_dir = r"output_Result_dir//" # Output dir where results will be saved
    error_dir = out_dir + "//Fail//" # Output subdir were the fail cases (where the model was wrong will be saved)
    model = "gpt-5"#, "human"

    run_test_text2im(code_main_dir=code_main_dir,
                    images_main_dir=images_main_dir,  # images main dir (Scitexture/images/
                    num_choices=10,  # number of choices in the multichoice tes
                    max_question=100,  # Max questions per text
                    num_samples=3,  # Number of referance image per model
                    mode="code",  # "description" # match images to code or to model description
                    error_dir=error_dir,  # error dir where mistakes will be saved
                    remove_code_comments=False,  # Remove comments from code
                    outdir=out_dir,  # output dir
                    model=model)


    #run_test_multi_model(code_main_dir=code_main_dir,images_main_dir=images_main_dir,main_outdir=out_dir)
