#Im2Code: Test LVLMS ability to associate images with the code and models that generate them

Test LVLMS ability to associate images with the code and models that generate them. This is done using  a multi choice question where the LVLM (GPT,Qwen,Gemini,Grok) receives code or model verbal description and several images generated by the model, as well as images generateby by different models, and is asked to find which images were generated by the described model or code.

The code runs on the [SciTextures dataset](https://zenodo.org/records/17485502)
For more details on the method, see: [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/pdf/2511.01817) 
If you are interested in directly inferring code and simulation, and models  from images, see the [Im2Sim repository.](https://github.com/sagieppel/Collecting-models-and-code-for-Visual-patterns-and-textures-generation-using-Agentic-AI)

​![](/scheme.jpg)

# How to use:

## 1. Download and extract the [SciTextures dataset from here](https://zenodo.org/records/17485502/files/Scitexture_Full_110K_images_jpg_format.zip?download=1)

## 2. Update your preferred API key at API_KEYS.py.  

​The code relies on API for major AI LVLM suppliers (GPT, Gemini, Qwen, DeepSeek, Claude, Grok...). You need to have API keys (and purchase tokens) for at least one of those (GPT-5  is recommended). 

The code supports APIs from OpenAI, Together.AI (Qwen, LLama), Grok,Gemini, and Claude. You can add other APIs to tools/ToolVisualQuestion.py.

## 3. Run test

In **Run_Test_im2model2code.py** in **__main__*** set path to the  SciTextures images and code main folders.

**code_main_dir = r"Scitexture/code_and_data/"  **

**images_main_dir = r"Scitexture/images/"** 

**out_dir = r"output_Result_dir//"**

​

And run the script, the results will be displayed on screen and saved in ***out_dir***.

